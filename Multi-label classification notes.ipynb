{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Label Classification Notes\n",
    "\n",
    "From https://www.youtube.com/watch?v=nNDqbUhtIRg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sklearn metrics package to measure efficacy of model\n",
    "# micro makes it predict rather than not predict at all\n",
    "\n",
    "f1 = f1_score(testY, y_pred, average='micro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach:\n",
    "1.  multi-class, multi-label problem\n",
    "2.  OneVsRestClassifier + SGD classifier\n",
    "3.  With 42k unique classes (tons of classes)\n",
    "4.  Train 42k classifiers to distinguish amongst known tags\n",
    "\n",
    "### SGD classifier:\n",
    "1. Quick and dirty\n",
    "2. Cheap computationally\n",
    "3. Good for more than 100k examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline:\n",
    "1.  Ingest Training Data\n",
    "2.  Build Feature Vector\n",
    "        2.1  3x title + body + tag parts\n",
    "        2.2  tfidf vectorizer from CBOW\n",
    "        2.3  200k x 100k sparse float for training\n",
    "        2.4  Fit with OneVsRestClassifier\n",
    "        2.5  Pickle Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'estimator__loss':('hinge', 'log', 'perceptron'),\n",
    "             'estimator__alpha':(0.1, 0.001, 0.00001, 0.0000001),\n",
    "             'estimator__penalty':('l1', 'l2', 'elasticnet')}\n",
    "\n",
    "classifier = OneVsRestClassifier( SGDClassifier( random_state = 0,\n",
    "                                               loss = 'hinge',\n",
    "                                               alpha = 0.00001,\n",
    "                                               penalty = 'elasticnet')).fit(features, labels)\n",
    "\n",
    "y_pred = classifier predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "1.  Intuition is that similarly tagged posts will group together\n",
    "2.  Necessarily the case, but still an interesting experiment\n",
    "3.  Turned out to be slower, no gain in accuracy\n",
    "        3.1 450 topics clustered with several hundred tags each\n",
    "        3.2 Expensive SVD step\n",
    "        3.3 Uneven clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = MiniBatchKMeans(init ='K-means++',\n",
    "                        n_clusters = NUM_CLUSTERS,\n",
    "                        n_init=30,\n",
    "                        batch_size=2000)\n",
    "\n",
    "clusterf = kmeans.fit(X, y)\n",
    "\n",
    "test_clusters = kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "Short for term frequency-inverse document frequency. It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. Is practical or weighting terms ina  bag of words and gaining intuition around what is important vs. what is less important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0.00009,\n",
    "                            max_features=200000,\n",
    "                            stop_words='english',\n",
    "                            smooth_idf=True,\n",
    "                            norm='l2',\n",
    "                            sublinear_tf=False,\n",
    "                            use_idf=True,\n",
    "                            ngram_range=(1,3))\n",
    "\n",
    "# ngram_range is really important here. If you start with 'windows xp crashed',\n",
    "# ngram(1,3) will product 6 features based on single words, bigrams, and trigrams.\n",
    "\n",
    "X = vectorizer.fit_transform(train_examples)\n",
    "X_test = vectorizer.transform(test_examples)\n",
    "\n",
    "# Can use this to look at different n-grams and get a sense for where \n",
    "# synonyms can be rolled in as synthetic data\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "1. Classifiers won't predict on every test item (where 'micro' comes into play)\n",
    "2. Predicting is better than not predicting (duh)\n",
    "3. stacked different classifiers\n",
    "        3.1 combined multiple methods for best predictions\n",
    "        3.2 didn't weight, sequenced for expediency\n",
    "        3.3 Wound up with 10 prediction files and merged them\n",
    "        3.4 dupes + 2 OvRClassifiers + Index + 5 Clusters + default\n",
    "        \n",
    "# Lessons:\n",
    "1. pre-analyze data\n",
    "2. reality check against literature and competitors\n",
    "3. avoid premature optimization\n",
    "4. save innovation and complexity until you have a great baseline score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notes from Machine Learning With Text\n",
    "From: https://www.youtube.com/watch?v=ZiKMIuYidY0\n",
    "\n",
    "1.  All features must be numeric, ML models can't deal with text explicitly\n",
    "2.  Every observation must have the same features in the same order\n",
    "3.  In order to make a prediction, the new observation must have the same features as the training observations, both in number and meaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the class\n",
    "from sklearn.neighbords import KNeighborsClassifier\n",
    "\n",
    "# Instantiate the model with default parameters\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit the model with data, no assignment\n",
    "knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using CountVectorizer to vectorize the vocabulary\n",
    "# Converts text into a matrix of token counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "simple_train = ['call you tonight', 'call me a cab', 'what you talking about willis?']\n",
    "\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Learns vocabulary of the training data\n",
    "vect.fit(simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about', 'cab', 'call', 'me', 'talking', 'tonight', 'what', 'willis', 'you']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examines the fitted vocabulary\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform training data into a 'document term matrix'\n",
    "simple_train_dtm = vect.transform(simple_train)\n",
    "simple_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert sparce matrix to a dense matrix\n",
    "simple_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What This is:\n",
    "\n",
    "Scheme for representing text as numerical data, where each individual token occurence frequency is treated as a feature. The vector of all the token frequencies or a given document is considered a multivariate sample.\n",
    "\n",
    "A corpus of documents can thus be represented by a matrix with one row per document and one column per token occurring in the corpus. We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the 'Bag of Words' or 'Bag of n-grams' representation.\n",
    "\n",
    "Bag of words does not keep track of order. Cannot use the matrix to reconstruct the messages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>talking</th>\n",
       "      <th>tonight</th>\n",
       "      <th>what</th>\n",
       "      <th>willis</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   about  cab  call  me  talking  tonight  what  willis  you\n",
       "0      0    0     1   0        0        1     0       0    1\n",
       "1      0    1     1   1        0        0     0       0    0\n",
       "2      1    0     0   0        1        0     1       1    1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(simple_train_dtm.toarray(), columns = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 8)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "# Show sparse matrix representation\n",
    "print(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Below:\n",
    "Does not have the relevant data set loaded. For context, lots of SMS that have been hand labeled as ham or spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converts labels to a numberical value\n",
    "sms['lable_num = sms.label.map({'ham':0, 'spam':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data needs to be a one dimensional object so we can pass it to count vectorizer\n",
    "X = sms.message\n",
    "y = sms.label_num\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import test_train_split\n",
    "\n",
    "# Uses sklearn to build a test/train split of the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_State = 1)\n",
    "\n",
    "# Check shapes\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize the vocabulary\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm\n",
    "\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Evaluating the Model:\n",
    "Using multinomial Naive Bayes:\n",
    "1.  Suitable for classification with discrete features (eg word counts for text classification).\n",
    "2.  The Multinomial distribution normally requires integer feature counts.\n",
    "3.  In practice, fractional counts such as tf-idf may work as well\n",
    "4.  Naive Bayes is really fast and great to start with until you know how much time you have for other approaches\n",
    "5.  For each and every token it calculates the conditional probability of a class given each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the model using the training data/sparse matrix\n",
    "# time magic command should give a rough estimate of runtime\n",
    "%time nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Makes class predictions for X_test_dtm\n",
    "y_pred_vlass = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the confusion matrix\n",
    "# upper left is true negative, bottom right is true positive, \n",
    "# upper right is false positive, bottom left is false negative\n",
    "\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to print the text message from false positives\n",
    "X_test[(y_pred_class==1) & (y_test==0)]\n",
    "\n",
    "# For false negatives\n",
    "X_test[(y_pred_class==0) & (y_test==1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
